controls:
version: rh-1.0
id: 1
text: "主节点安全配置"
type: "master"
groups:
  - id: 1.1
    text: "主节点配置文件"
    checks:
      - id: 1.1.1
        text: "确保将 API 服务器 Pod 规范文件的权限设置为 644 或更严格 (手动)。"
        audit: |
          # Get the node name where the pod is running
          NODE_NAME=$(oc get pod "$HOSTNAME" -o=jsonpath='{.spec.nodeName}')

          # Get the pod name in the openshift-kube-apiserver namespace
          POD_NAME=$(oc get pods -n openshift-kube-apiserver -l app=openshift-kube-apiserver --field-selector spec.nodeName="$NODE_NAME" -o jsonpath='{.items[0].metadata.name}' 2>/dev/null)

          if [ -z "$POD_NAME" ]; then
              echo "No matching pods found on the current node."
          else
             # Execute the stat command
             oc exec -n openshift-kube-apiserver "$POD_NAME" -- stat -c "$POD_NAME %n permissions=%a" /etc/kubernetes/static-pod-resources/kube-apiserver-pod.yaml
          fi
        use_multiple_values: true
        tests:
          test_items:
            - flag: "permissions"
              compare:
                op: bitmask
                value: "644"
        remediation: |-
          无需补救措施；文件权限由操作员管理。
        scored: false

      - id: 1.1.2
        text: "确保 API 服务器 Pod 规范文件的所有权设置为 root:root（手动）。"
        audit: |
          # Get the node name where the pod is running
          NODE_NAME=$(oc get pod "$HOSTNAME" -o=jsonpath='{.spec.nodeName}')

          # Get the pod name in the openshift-kube-apiserver namespace
          POD_NAME=$(oc get pods -n openshift-kube-apiserver -l app=openshift-kube-apiserver --field-selector spec.nodeName="$NODE_NAME" -o jsonpath='{.items[0].metadata.name}' 2>/dev/null)

          if [ -z "$POD_NAME" ]; then
              echo "No matching pods found on the current node."
          else
             # Execute the stat command
             oc exec -n openshift-kube-apiserver "$POD_NAME" -- stat -c "$POD_NAME %n %U:%G" /etc/kubernetes/static-pod-resources/kube-apiserver-pod.yaml
          fi
        use_multiple_values: true
        tests:
          test_items:
            - flag: "root:root"
        remediation: |-
          无需进行补救措施；文件权限由运维人员管理。
        scored: false

      - id: 1.1.3
        text: "确保控制器管理器 Pod 规范文件的权限设置为 644 或更严格（手动）。"
        audit: |
          # Get the node name where the pod is running
          NODE_NAME=$(oc get pod "$HOSTNAME" -o=jsonpath='{.spec.nodeName}')

          # Get the pod name in the openshift-kube-controller-manager namespace
          POD_NAME=$(oc get pods -n openshift-kube-controller-manager -l app=kube-controller-manager --field-selector spec.nodeName="$NODE_NAME" -o jsonpath='{.items[0].metadata.name}' 2>/dev/null)

          if [ -z "$POD_NAME" ]; then
             echo "No matching pods found on the current node."
          else
            # Execute the stat command
            oc exec -n openshift-kube-controller-manager "$POD_NAME" -- stat -c "$POD_NAME %n permissions=%a" /etc/kubernetes/static-pod-resources/kube-controller-manager-pod.yaml
          fi
        use_multiple_values: true
        tests:
          test_items:
            - flag: "permissions"
              compare:
                op: bitmask
                value: "644"
        remediation: |-
          无需进行补救；文件权限由操作员管理。
        scored: false

      - id: 1.1.4
        text: "确保控制器管理器 Pod 规范文件的所有权设置为 root:root（手动）。"
        audit: |
          # Get the node name where the pod is running
          NODE_NAME=$(oc get pod "$HOSTNAME" -o=jsonpath='{.spec.nodeName}')

          # Get the pod name in the openshift-kube-controller-manager namespace
          POD_NAME=$(oc get pods -n openshift-kube-controller-manager -l app=kube-controller-manager --field-selector spec.nodeName="$NODE_NAME" -o jsonpath='{.items[0].metadata.name}' 2>/dev/null)

          if [ -z "$POD_NAME" ]; then
            echo "No matching pods found on the current node."
          else
           # Execute the stat command
           oc exec -n openshift-kube-controller-manager "$POD_NAME" -- stat -c "$POD_NAME %n %U:%G" /etc/kubernetes/static-pod-resources/kube-controller-manager-pod.yaml
          fi
        use_multiple_values: true
        tests:
          test_items:
            - flag: "root:root"
        remediation: |-
          无需进行补救操作；文件权限由操作员管理。
        scored: false

      - id: 1.1.5
        text: "确保调度器 Pod 规范文件的权限设置为 644 或更严格（手动）。"
        audit: |
          # Get the node name where the pod is running
          NODE_NAME=$(oc get pod "$HOSTNAME" -o=jsonpath='{.spec.nodeName}')

          # Get the pod name in the openshift-kube-scheduler namespace
          POD_NAME=$(oc get pods -n openshift-kube-scheduler -l app=openshift-kube-scheduler --field-selector spec.nodeName="$NODE_NAME" -o jsonpath='{.items[0].metadata.name}' 2>/dev/null)

          if [ -z "$POD_NAME" ]; then
           echo "No matching pods found on the current node."
          else
          # Execute the stat command
          oc exec -n openshift-kube-scheduler "$POD_NAME" -- stat -c "$POD_NAME %n permissions=%a" /etc/kubernetes/static-pod-resources/kube-scheduler-pod.yaml
          fi
        use_multiple_values: true
        tests:
          test_items:
            - flag: "permissions"
              compare:
                op: bitmask
                value: "644"
        remediation: |-
          无需进行修复操作；文件权限由操作者管理。
        scored: false

      - id: 1.1.6
        text: "确保调度器 Pod 规范文件的所有权设置为 root:root (手动)。"
        audit: |
          # Get the node name where the pod is running
          NODE_NAME=$(oc get pod "$HOSTNAME" -o=jsonpath='{.spec.nodeName}')

          # Get the pod name in the openshift-kube-scheduler namespace
          POD_NAME=$(oc get pods -n openshift-kube-scheduler -l app=openshift-kube-scheduler --field-selector spec.nodeName="$NODE_NAME" -o jsonpath='{.items[0].metadata.name}' 2>/dev/null)

          if [ -z "$POD_NAME" ]; then
           echo "No matching pods found on the current node."
          else
          # Execute the stat command
          oc exec -n openshift-kube-scheduler "$POD_NAME" -- stat -c "$POD_NAME %n %U:%G" /etc/kubernetes/static-pod-resources/kube-scheduler-pod.yaml
          fi
        use_multiple_values: true
        tests:
          test_items:
            - flag: "root:root"
        remediation: |-
          无需进行修复操作；文件权限由操作员管理。
        scored: false

      - id: 1.1.7
        text: "确保 etcd pod 规范文件的权限设置为 644 或更严格（手动）。"
        audit: |
          # Get the node name where the pod is running
          NODE_NAME=$(oc get pod "$HOSTNAME" -o=jsonpath='{.spec.nodeName}')

          # Get the pod name in the openshift-etcd namespace
          POD_NAME=$(oc get pods -n openshift-etcd -l app=etcd --field-selector spec.nodeName="$NODE_NAME" -o jsonpath='{.items[0].metadata.name}' 2>/dev/null)

          if [ -z "$POD_NAME" ]; then
           echo "No matching pods found on the current node."
          else
          # Execute the stat command
          oc rsh -n openshift-etcd "$POD_NAME" stat -c "$POD_NAME %n permissions=%a" /etc/kubernetes/manifests/etcd-pod.yaml
          fi
        use_multiple_values: true
        tests:
          test_items:
            - flag: "permissions"
              compare:
                op: bitmask
                value: "644"
        remediation: |-
          无需进行修复操作；文件权限由操作员管理。
        scored: false

      - id: 1.1.8
        text: "确保 etcd pod 规范文件的所有权设置为 root:root（手动）。"
        audit: |
          # Get the node name where the pod is running
          NODE_NAME=$(oc get pod "$HOSTNAME" -o=jsonpath='{.spec.nodeName}')

          # Get the pod name in the openshift-etcd namespace
          POD_NAME=$(oc get pods -n openshift-etcd -l app=etcd --field-selector spec.nodeName="$NODE_NAME" -o jsonpath='{.items[0].metadata.name}' 2>/dev/null)

          if [ -z "$POD_NAME" ]; then
           echo "No matching pods found on the current node."
          else
          # Execute the stat command
          oc rsh -n openshift-etcd "$POD_NAME" stat -c "$POD_NAME %n %U:%G" /etc/kubernetes/manifests/etcd-pod.yaml
          fi
        use_multiple_values: true
        tests:
          test_items:
            - flag: "root:root"
        remediation: |-
          无需补救；文件权限由操作员管理。
        scored: false

      - id: 1.1.9
        text: "确保将容器网络接口文件的权限设置为 644 或者更严格（手动）。"
        audit: |
          # Get the node name where the pod is running
          NODE_NAME=$(oc get pod "$HOSTNAME" -o=jsonpath='{.spec.nodeName}')
          # For CNI multus
          # Get the pod name in the openshift-multus namespace
          POD_NAME=$(oc get pods -n openshift-multus -l app=multus --field-selector spec.nodeName="$NODE_NAME" -o jsonpath='{.items[0].metadata.name}' 2>/dev/null)

          if [ -z "$POD_NAME" ]; then
           echo "No matching pods found on the current node."
          else
          # Execute the stat command
          oc exec -n openshift-multus "$POD_NAME"  -- /bin/bash -c "stat -c \"$i %n permissions=%a\" /host/etc/cni/net.d/*.conf";  2>/dev/null
          oc exec -n openshift-multus "$POD_NAME"  -- /bin/bash -c "stat -c \"$i %n permissions=%a\" /host/var/run/multus/cni/net.d/*.conf";  2>/dev/null
          fi
          # For SDN pods
          POD_NAME=$(oc get pods -n openshift-sdn -l app=sdn --field-selector spec.nodeName="$NODE_NAME" -o jsonpath='{.items[0].metadata.name}' 2>/dev/null)

          if [ -z "$POD_NAME" ]; then
           echo "No matching pods found on the current node."
          else
          # Execute the stat command
          oc exec -n openshift-sdn "$POD_NAME"  -- find /var/lib/cni/networks/openshift-sdn -type f -exec stat -c "$i %n permissions=%a" {} \; 2>/dev/null
          oc exec -n openshift-sdn "$POD_NAME"  -- find /var/run/openshift-sdn -type f -exec stat -c "$i %n permissions=%a" {} \; 2>/dev/null
          fi

          # For OVS pods
          POD_NAME=$(oc get pods -n openshift-sdn -l app=ovs --field-selector spec.nodeName="$NODE_NAME" -o jsonpath='{.items[0].metadata.name}' 2>/dev/null)

          if [ -z "$POD_NAME" ]; then
           echo "No matching pods found on the current node."
          else
          # Execute the stat command
          oc exec -n openshift-sdn "$POD_NAME"  -- find /var/run/openvswitch -type f -exec stat -c "$i %n permissions=%a" {} \; 2>/dev/null
          oc exec -n openshift-sdn "$POD_NAME"  -- find /etc/openvswitch -type f -exec stat -c "$i %n permissions=%a" {} \; 2>/dev/null
          oc exec -n openshift-sdn "$POD_NAME"  -- find /run/openvswitch -type f -exec stat -c "$i %n permissions=%a" {} \; 2>/dev/null
          fi
        use_multiple_values: true
        tests:
          test_items:
            - flag: "permissions"
              compare:
                op: bitmask
                value: "644"
        remediation: |-
          无需补救措施；文件权限由操作员管理。
        scored: false

      - id: 1.1.10
        text: "确保容器网络接口文件的所有权设置为 root:root（手动）。"
        audit: |
          # Get the node name where the pod is running
          NODE_NAME=$(oc get pod "$HOSTNAME" -o=jsonpath='{.spec.nodeName}')
          # For CNI multus
          # Get the pod name in the openshift-multus namespace
          POD_NAME=$(oc get pods -n openshift-multus -l app=multus --field-selector spec.nodeName="$NODE_NAME" -o jsonpath='{.items[0].metadata.name}' 2>/dev/null)

          if [ -z "$POD_NAME" ]; then
           echo "No matching pods found on the current node."
          else
          # Execute the stat command
          oc exec -n openshift-multus "$POD_NAME" -- /bin/bash -c "stat -c '$i %n %U:%G' /host/etc/cni/net.d/*.conf" 2>/dev/null
          oc exec -n openshift-multus $i -- /bin/bash -c "stat -c '$i %n %U:%G' /host/var/run/multus/cni/net.d/*.conf"  2>/dev/null
          fi
          # For SDN pods
          POD_NAME=$(oc get pods -n openshift-sdn -l app=sdn --field-selector spec.nodeName="$NODE_NAME" -o jsonpath='{.items[0].metadata.name}' 2>/dev/null)

          if [ -z "$POD_NAME" ]; then
           echo "No matching pods found on the current node."
          else
          # Execute the stat command
          oc exec -n openshift-sdn "$POD_NAME"  -- find /var/lib/cni/networks/openshift-sdn -type f -exec stat -c "$i %n %U:%G" {} \; 2>/dev/null
          oc exec -n openshift-sdn "$POD_NAME"  -- find /var/run/openshift-sdn -type f -exec stat -c "$i %n %U:%G" {} \; 2>/dev/null
          fi
          # For OVS pods in 4.5
          POD_NAME=$(oc get pods -n openshift-sdn -l app=ovs --field-selector spec.nodeName="$NODE_NAME" -o jsonpath='{.items[0].metadata.name}' 2>/dev/null)

          if [ -z "$POD_NAME" ]; then
           echo "No matching pods found on the current node."
          else
          # Execute the stat command
          oc exec -n openshift-sdn "$POD_NAME"  -- find /var/run/openvswitch -type f -exec stat -c "$i %n %U:%G" {} \; 2>/dev/null
          oc exec -n openshift-sdn "$POD_NAME"  -- find /etc/openvswitch -type f -exec stat -c "$i %n %U:%G" {} \; 2>/dev/null
          oc exec -n openshift-sdn "$POD_NAME"  -- find /run/openvswitch -type f -exec stat -c "$i %n %U:%G" {} \; 2>/dev/null
          fi
        use_multiple_values: true
        tests:
          test_items:
            - flag: "root:root"
        remediation: |-
          无需纠正；文件权限由操作者管理。
        scored: false

      - id: 1.1.11
        text: "确保 etcd 数据目录的权限设置为 700 或更严格（手动）。"
        audit: |
          # Get the node name where the pod is running
          NODE_NAME=$(oc get pod "$HOSTNAME" -o=jsonpath='{.spec.nodeName}')

          # Get the pod name in the openshift-etcd namespace
          POD_NAME=$(oc get pods -n openshift-etcd -l app=etcd --field-selector spec.nodeName="$NODE_NAME" -o jsonpath='{.items[0].metadata.name}' 2>/dev/null)

          if [ -z "$POD_NAME" ]; then
          echo "No matching pods found on the current node."
          else
          # Execute the stat command
          oc exec -n openshift-etcd "$POD_NAME" -- stat -c "$POD_NAME %n permissions=%a" /var/lib/etcd/member
          fi
        use_multiple_values: true
        tests:
          test_items:
            - flag: "permissions"
              compare:
                op: bitmask
                value: "700"
        remediation: |-
          无需进行修复；文件权限由操作员管理。
        scored: false

      - id: 1.1.12
        text: "确保将 etcd 数据目录的所有权设置为 etcd:etcd（手动）。"
        audit: |
          # Get the node name where the pod is running
          NODE_NAME=$(oc get pod "$HOSTNAME" -o=jsonpath='{.spec.nodeName}')

          # Get the pod name in the openshift-etcd namespace
          POD_NAME=$(oc get pods -n openshift-etcd -l app=etcd --field-selector spec.nodeName="$NODE_NAME" -o jsonpath='{.items[0].metadata.name}' 2>/dev/null)

          if [ -z "$POD_NAME" ]; then
          echo "No matching pods found on the current node."
          else
          # Execute the stat command
          oc exec -n openshift-etcd "$POD_NAME" -- stat -c "$POD_NAME %n %U:%G" /var/lib/etcd/member
          fi
        use_multiple_values: true
        tests:
          test_items:
            - flag: "root:root"
        remediation: |-
          无需进行补救措施；文件权限由操作员管理。
        scored: false

      - id: 1.1.13
        text: "确保 admin.conf 文件的权限设置为 644 或更为严格（手动）。"
        audit: |
          NODE_NAME=$(oc get pod $HOSTNAME -o=jsonpath='{.spec.nodeName}')
          oc debug node/$NODE_NAME -- chroot /host stat -c "$NODE_NAME %n permissions=%a" /etc/kubernetes/kubeconfig 2> /dev/null
        use_multiple_values: true
        tests:
          test_items:
            - flag: "permissions"
              compare:
                op: bitmask
                value: "644"
        remediation: |-
          无需进行修复；文件权限由操作员管理。
        scored: false

      - id: 1.1.14
        text: "确保 admin.conf 文件的所有权设置为 root:root（手动）."
        audit: |
          NODE_NAME=$(oc get pod $HOSTNAME -o=jsonpath='{.spec.nodeName}')
          oc debug node/$NODE_NAME -- chroot /host stat -c "$NODE_NAME %n %U:%G" /etc/kubernetes/kubeconfig 2> /dev/null
        use_multiple_values: true
        tests:
          test_items:
            - flag: "root:root"
        remediation: |-
          无需进行修复；文件权限由操作员管理。
        scored: false

      - id: 1.1.15
        text: "确保 scheduler.conf 文件的权限设置为 644 或更严格（手动）。"
        audit: |
          # Get the node name where the pod is running
          NODE_NAME=$(oc get pod "$HOSTNAME" -o=jsonpath='{.spec.nodeName}')

          # Get the pod name in the openshift-kube-scheduler namespace
          POD_NAME=$(oc get pods -n openshift-kube-scheduler -l app=openshift-kube-scheduler --field-selector spec.nodeName="$NODE_NAME" -o jsonpath='{.items[0].metadata.name}' 2>/dev/null)

          if [ -z "$POD_NAME" ]; then
          echo "No matching pods found on the current node."
          else
          # Execute the stat command
          oc exec -n openshift-kube-scheduler "$POD_NAME" -- stat -c "$POD_NAME %n permissions=%a" /etc/kubernetes/static-pod-resources/configmaps/scheduler-kubeconfig/kubeconfig
          fi
        use_multiple_values: true
        tests:
          test_items:
            - flag: "permissions"
              compare:
                op: bitmask
                value: "644"
        remediation: |-
          无需进行修复操作；文件权限由运维人员管理。
        scored: false

      - id: 1.1.16
        text: "确保 scheduler.conf 文件的所有权设置为 root:root（手动）."
        audit: |
          # Get the node name where the pod is running
          NODE_NAME=$(oc get pod "$HOSTNAME" -o=jsonpath='{.spec.nodeName}')

          # Get the pod name in the openshift-kube-scheduler namespace
          POD_NAME=$(oc get pods -n openshift-kube-scheduler -l app=openshift-kube-scheduler --field-selector spec.nodeName="$NODE_NAME" -o jsonpath='{.items[0].metadata.name}' 2>/dev/null)

          if [ -z "$POD_NAME" ]; then
          echo "No matching pods found on the current node."
          else
          # Execute the stat command
          oc exec -n openshift-kube-scheduler "$POD_NAME" -- stat -c "$POD_NAME %n %U:%G" /etc/kubernetes/static-pod-resources/configmaps/scheduler-kubeconfig/kubeconfig
          fi
        use_multiple_values: true
        tests:
          test_items:
            - flag: "root:root"
        remediation: |-
          无需进行修复操作；文件权限由操作员管理。
        scored: false

      - id: 1.1.17
        text: "确保 controller-manager.conf 文件的权限设置为 644 或更严格（手动）。"
        audit: |
          # Get the node name where the pod is running
          NODE_NAME=$(oc get pod "$HOSTNAME" -o=jsonpath='{.spec.nodeName}')

          # Get the pod name in the openshift-kube-controller-manager namespace
          POD_NAME=$(oc get pods -n openshift-kube-controller-manager -l app=kube-controller-manager --field-selector spec.nodeName="$NODE_NAME" -o jsonpath='{.items[0].metadata.name}' 2>/dev/null)

          if [ -z "$POD_NAME" ]; then
          echo "No matching pods found on the current node."
          else
          # Execute the stat command
          oc exec -n openshift-kube-controller-manager "$POD_NAME" -- stat -c "$POD_NAME %n permissions=%a" /etc/kubernetes/static-pod-resources/configmaps/controller-manager-kubeconfig/kubeconfig
          fi
        use_multiple_values: true
        tests:
          test_items:
            - flag: "permissions"
              compare:
                op: bitmask
                value: "644"
        remediation: |-
          无需进行修复操作；文件权限由操作者管理。
        scored: false

      - id: 1.1.18
        text: "确保 controller-manager.conf 文件的所有权设置为 root:root（手动）。"
        audit: |
          # Get the node name where the pod is running
          NODE_NAME=$(oc get pod "$HOSTNAME" -o=jsonpath='{.spec.nodeName}')

          # Get the pod name in the openshift-kube-controller-manager namespace
          POD_NAME=$(oc get pods -n openshift-kube-controller-manager -l app=kube-controller-manager --field-selector spec.nodeName="$NODE_NAME" -o jsonpath='{.items[0].metadata.name}' 2>/dev/null)

          if [ -z "$POD_NAME" ]; then
          echo "No matching pods found on the current node."
          else
          # Execute the stat command
          oc exec -n openshift-kube-controller-manager "$POD_NAME" -- stat -c "$POD_NAME %n %U:%G" /etc/kubernetes/static-pod-resources/configmaps/controller-manager-kubeconfig/kubeconfig
          fi
        use_multiple_values: true
        tests:
          test_items:
            - flag: "root:root"
        remediation: |-
          不需要修复措施；文件权限由操作员管理。
        scored: false

      - id: 1.1.19
        text: "确保 Kubernetes PKI 目录和文件的所有权设置为 root:root（手动）。"
        audit: |
          # Should return root:root for all files and directories
          # Get the node name where the pod is running
          NODE_NAME=$(oc get pod "$HOSTNAME" -o=jsonpath='{.spec.nodeName}')

          # Get the pod name in the openshift-kube-controller-manager namespace
          POD_NAME=$(oc get pods -n openshift-kube-apiserver -l app=openshift-kube-apiserver --field-selector spec.nodeName="$NODE_NAME" -o jsonpath='{.items[0].metadata.name}' 2>/dev/null)

          if [ -z "$POD_NAME" ]; then
          echo "No matching pods found on the current node."
          else
          # echo $i static-pod-certs
          oc exec -n openshift-kube-apiserver "$POD_NAME" -c kube-apiserver -- find /etc/kubernetes/static-pod-certs -type d -wholename '*/secrets*' -exec stat -c "$i %n %U:%G" {} \;
          oc exec -n openshift-kube-apiserver "$POD_NAME" -c kube-apiserver -- find /etc/kubernetes/static-pod-certs -type f -wholename '*/secrets*' -exec stat -c "$i %n %U:%G" {} \;
          # echo $i static-pod-resources
          oc exec -n openshift-kube-apiserver "$POD_NAME" -c kube-apiserver -- find /etc/kubernetes/static-pod-resources -type d -wholename '*/secrets*' -exec stat -c "$i %n %U:%G" {} \;
          oc exec -n openshift-kube-apiserver "$POD_NAME" -c kube-apiserver -- find /etc/kubernetes/static-pod-resources -type f -wholename '*/secrets*' -exec stat -c "$i %n %U:%G" {} \;
          fi
        use_multiple_values: true
        tests:
          test_items:
            - flag: "root:root"
        remediation: |-
          不需要进行修复操作；文件权限由运营商管理。
        scored: false

      - id: 1.1.20
        text: "确保 OpenShift PKI 证书文件的权限设置为 644 或更严格（手动）。"
        audit: |
          # Get the node name where the pod is running
          NODE_NAME=$(oc get pod "$HOSTNAME" -o=jsonpath='{.spec.nodeName}')

          # Get the pod name in the openshift-kube-apiserver namespace
          POD_NAME=$(oc get pods -n openshift-kube-apiserver -l app=openshift-kube-apiserver --field-selector spec.nodeName="$NODE_NAME" -o jsonpath='{.items[0].metadata.name}' 2>/dev/null)

          if [ -z "$POD_NAME" ]; then
          echo "No matching pods found on the current node."
          else
          # Execute the stat command
          oc exec -n openshift-kube-apiserver "$POD_NAME" -c kube-apiserver -- find /etc/kubernetes/static-pod-certs -type f -wholename '*/secrets/*.crt' -exec stat -c "$POD_NAME %n permissions=%a" {} \;
          fi
        use_multiple_values: true
        tests:
          test_items:
            - flag: "permissions"
              compare:
                op: bitmask
                value: "644"
        remediation: |-
          无需进行修复；文件权限由操作员管理。
        scored: false

      - id: 1.1.21
        text: "确保 OpenShift PKI 密钥文件的权限设置为 600（手动）。"
        audit: |
          # Get the node name where the pod is running
          NODE_NAME=$(oc get pod "$HOSTNAME" -o=jsonpath='{.spec.nodeName}')

          # Get the pod name in the openshift-kube-apiserver namespace
          POD_NAME=$(oc get pods -n openshift-kube-apiserver -l app=openshift-kube-apiserver --field-selector spec.nodeName="$NODE_NAME" -o jsonpath='{.items[0].metadata.name}' 2>/dev/null)

          if [ -z "$POD_NAME" ]; then
          echo "No matching pods found on the current node."
          else
          # Execute the stat command
          oc exec -n openshift-kube-apiserver "$POD_NAME" -c kube-apiserver -- find /etc/kubernetes/static-pod-certs -type f -wholename '*/secrets/*.key' -exec stat -c "$POD_NAME %n permissions=%a" {} \;
          fi
        use_multiple_values: true
        tests:
          test_items:
            - flag: "permissions"
              compare:
                op: bitmask
                value: "600"
        remediation: |-
          无需补救措施；文件权限由操作员管理。
        scored: false

  - id: 1.2
    text: "API 服务器"
    checks:
      - id: 1.2.1
        text: "确保匿名请求被授权（手动）。"
        audit: |
          # To verify that userGroups include system:unauthenticated
          oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | jq '.auditConfig.policyConfiguration.rules[]?'
          # To verify that userGroups include system:unauthenticated
          oc get configmap config -n openshift-apiserver -ojson | jq -r '.data["config.yaml"]' | jq '.auditConfig.policyConfiguration.rules[]?.userGroups'
          # To verify RBAC is enabled
          oc get clusterrolebinding
          oc get clusterrole
          oc get rolebinding
          oc get role
        tests:
          test_items:
            - flag: "system:unauthenticated"
        remediation: |-
          无需任何操作。默认配置不应被修改。
        scored: false

      - id: 1.2.2
        text: "确保未设置 --basic-auth-file 参数（手动）。"
        audit: |
          oc -n openshift-kube-apiserver get cm config -o yaml | grep --color "basic-auth"
          oc -n openshift-apiserver get cm config -o yaml | grep --color "basic-auth"
          # Add | awk '$3 != "AVAILABLE" { if ($3){print "available=true"}else{print "available=false"} }; to create AVAILABLE = true/false form
          oc get clusteroperator authentication | awk '$3 != "AVAILABLE" { if ($3){print "available=true"}else{print "available=false"} }'
        tests:
          bin_op: and
          test_items:
            - flag: "basic-auth-file"
              set: false
            - flag: "available"
              compare:
                op: eq
                value: true
        remediation: |-
          不需要任何配置。在 OpenShift 上无法配置 --basic-auth-file。
        scored: false

      - id: 1.2.3
        text: "确保 --token-auth-file 参数未设置（手动）。"
        audit: |
          # Verify that the token-auth-file flag is not present
          oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | jq '.apiServerArguments'
          oc get configmap config -n openshift-apiserver -ojson | jq -r '.data["config.yaml"]' | jq '.apiServerArguments'
          oc get kubeapiservers.operator.openshift.io cluster -o json | jq '.spec.observedConfig.apiServerArguments'
          #Verify that the authentication operator is running
          oc get clusteroperator authentication | awk '$3 != "AVAILABLE" { if ($3){print "available=true"}else{print "available=false"} }'
        tests:
          bin_op: and
          test_items:
            - flag: "token-auth-file"
              set: false
            - flag: "available"
              compare:
                op: eq
                value: true
        remediation: |-
          不需要任何东西。
        scored: false

      - id: 1.2.4
        text: "使用 https 进行 kubelet 连接（手动配置）。"
        audit: |
          #for 4.5
          oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | jq '.kubeletClientInfo'
          #for 4.6
          oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | jq '.apiServerArguments'
          #for both 4.5 and 4.6
          oc -n openshift-apiserver describe secret serving-cert
        tests:
          bin_op: and
          test_items:
            - flag: "/etc/kubernetes/static-pod-resources/secrets/kubelet-client/tls.crt"
            - flag: "/etc/kubernetes/static-pod-resources/secrets/kubelet-client/tls.key"
        remediation: |-
          无需进行修复。OpenShift 平台组件使用 X.509 证书进行身份验证。OpenShift 管理平台组件的 CA 证书和普通证书。这是不可配置的。
        scored: false

      - id: 1.2.5
        text: "确保 kubelet 使用证书进行身份验证（手动）。"
        audit: |
          #for 4.5
          oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | jq '.kubeletClientInfo'
          #for 4.6
          oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | jq '.apiServerArguments'
          #for both 4.5 and 4.6
          oc -n openshift-apiserver describe secret serving-cert
        tests:
          bin_op: and
          test_items:
            - flag: "/etc/kubernetes/static-pod-resources/secrets/kubelet-client/tls.crt"
            - flag: "/etc/kubernetes/static-pod-resources/secrets/kubelet-client/tls.key"
        remediation: |-
          无需进行修复。OpenShift 平台组件使用 X.509 证书进行身份验证。OpenShift 管理平台组件的 CA 和证书。这是不可配置的。
        scored: false

      - id: 1.2.6
        text: "验证 kubelet 证书颁发机构是否设置正确（手动）。"
        audit: |
          # for 4.5
          oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | jq '.kubeletClientInfo'
          # for 4.6
          oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | jq '.apiServerArguments'
        tests:
          test_items:
            - flag: "/etc/kubernetes/static-pod-resources/configmaps/kubelet-serving-ca/ca-bundle.crt"
        remediation: |-
          无需进行修复。OpenShift 平台组件使用 X.509 证书进行身份验证。OpenShift 管理平台组件的 CA 和证书。这项操作不可配置。
        scored: false

      - id: 1.2.7
        text: "确保 --authorization-mode 参数未设置为 AlwaysAllow（手动）。"
        audit: |
          # To verify that the authorization-mode argument is not used
          oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | jq '.apiServerArguments'
          oc get configmap config -n openshift-apiserver -ojson | jq -r '.data["config.yaml"]' | jq '.apiServerArguments'
          #Check that no overrides are configured
          oc get kubeapiservers.operator.openshift.io cluster -o json | jq -r '.spec.unsupportedConfigOverrides'
          # To verify RBAC is configured:
          oc get clusterrolebinding
          oc get clusterrole
          oc get rolebinding
          oc get role
        audit_config: |
          oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | jq '.apiServerArguments'
        tests:
          bin_op: or
          test_items:
            - path: "{.authorization-mode}"
              compare:
                op: nothave
                value: "AlwaysAllow"
            - path: "{.authorization-mode}"
              flag: "authorization-mode"
              set: false
        remediation: |-
          RBAC 一直是开启的，OpenShift API 服务器不使用分配给标志 authorization-mode 的值。
        scored: false

      - id: 1.2.8
        text: "验证 Node 授权器已启用（手动）。"
        audit: |
          # For OCP 4.5 and earlier verify that authorization-mode is not used
          oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | jq '.apiServerArguments'
          oc get kubeapiservers.operator.openshift.io cluster -o json | jq '.spec.observedConfig.apiServerArguments'
          # For OCP 4.5 and earlier verify that authorization-mode is not used
          NODE_NAME=$(oc get pod $HOSTNAME -o=jsonpath='{.spec.nodeName}')
          oc debug node/$NODE_NAME -- chroot /host cat /etc/kubernetes/kubelet.conf | grep authorization-mode 2> /dev/null
          oc debug node/$NODE_NAME -- chroot /host ps -aux | grep kubelet | grep authorization-mode 2> /dev/null
          #Check that no overrides are configured
          oc get kubeapiservers.operator.openshift.io cluster -o json | jq -r '.spec.unsupportedConfigOverrides'
        audit_config: |
          oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | jq '.apiServerArguments'
        tests:
          bin_op: or
          test_items:
            - path: "{.authorization-mode}"
              compare:
                op: has
                value: "Node"
            - path: "{.authorization-mode}"
              flag: "authorization-mode"
              set: false
        remediation: |-
          无需进行修复。
        scored: false

      - id: 1.2.9
        text: "验证 RBAC 是否已启用（手动）"
        audit: |
          # For 4.5 To verify that the authorization-mode argument is not used
          oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | jq '.apiServerArguments'
          oc get configmap config -n openshift-apiserver -ojson | jq -r '.data["config.yaml"]' | jq '.apiServerArguments'
          #Check that no overrides are configured
          oc get kubeapiservers.operator.openshift.io cluster -o json | jq -r '.spec.unsupportedConfigOverrides'
          # To verify RBAC is used
          oc get clusterrolebinding
          oc get clusterrole
          oc get rolebinding
          oc get role
          # For 4.6, verify that the authorization-mode argument includes RBAC
        audit_config: |
          oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | jq '.apiServerArguments'
        tests:
          bin_op: or
          test_items:
            - path: "{.authorization-mode}"
              compare:
                op: has
                value: "RBAC"
            - path: "{.authorization-mode}"
              flag: "authorization-mode"
              set: false
        remediation: |-
          无法禁用 RBAC。
        scored: false

      - id: 1.2.10
        text: "确保 APIPriorityAndFairness 功能门已启用 (手动)"
        audit: |
          #Verify the APIPriorityAndFairness feature-gate
          oc get kubeapiservers.operator.openshift.io cluster -o json | jq '.spec.observedConfig.apiServerArguments'
          #Verify the set of admission-plugins for OCP 4.6 and higher
          oc -n openshift-kube-apiserver get configmap config -o json | jq -r '.data."config.yaml"' | jq '.apiServerArguments."enable-admission-plugins"'
          #Check that no overrides are configured
          oc get kubeapiservers.operator.openshift.io cluster -o json | jq -r '.spec.unsupportedConfigOverrides'
        tests:
          bin_op: and
          test_items:
            - flag: "APIPriorityAndFairness=true"
            - flag: "EventRateLimit"
              set: false
        remediation: |-
          无需进行修复。
        scored: false

      - id: 1.2.11
        text: "确保拒绝控制插件 AlwaysAdmit 未设置（手动）。"
        audit: |
          #Verify the set of admission-plugins for OCP 4.6 and higher
          oc -n openshift-kube-apiserver get configmap config -o json | jq -r '.data."config.yaml"' | jq '.apiServerArguments."enable-admission-plugins"'
          #Check that no overrides are configured
          oc get kubeapiservers.operator.openshift.io cluster -o json | jq -r '.spec.unsupportedConfigOverrides'
        tests:
          test_items:
            - flag: "AlwaysAdmit"
              set: false
        remediation: |-
          不需要进行任何修复。在 OpenShift 中无法启用 AlwaysAdmit 准入控制器。
        scored: false

      - id: 1.2.12
        text: "确保 admission 控制插件 AlwaysPullImages 被设置为 (手动)。"
        audit: |
          #Verify the set of admissi    on-plugins for OCP 4.6 and higher
          oc -n openshift-kube-apiserver get configmap config -o json | jq -r '.data."config.yaml"' | jq '.apiServerArguments."enable-admission-plugins"'
          #Check that no overrides are configured
          oc get kubeapiservers.operator.openshift.io cluster -o json | jq -r '.spec.unsupportedConfigOverrides'
        tests:
          test_items:
            - flag: "AlwaysPullImages"
              set: false
        remediation: |-
          不需要。
        scored: false

      - id: 1.2.13
        text: "确保 admission 控制插件 SecurityContextDeny 未设置（手动）。"
        audit: |
          #Verify the set of admission-plugins for OCP 4.6 and higher
          oc -n openshift-kube-apiserver get configmap config -o json | jq -r '.data."config.yaml"' | jq '.apiServerArguments."enable-admission-plugins"'
          output=$(oc -n openshift-kube-apiserver get configmap config -o json | jq -r '.data."config.yaml"' | jq '.apiServerArguments."enable-admission-plugins"')
          [ "$output" == "null" ] && echo "ocp 4.5 has SecurityContextDeny and SecurityContextConstraint compiled" || echo $output
          #Check that no overrides are configured
          oc get kubeapiservers.operator.openshift.io cluster -o json | jq -r '.spec.unsupportedConfigOverrides'
          #Verify that SecurityContextConstraints are deployed
          oc get scc
          oc describe scc restricted
        tests:
          bin_op: and
          test_items:
            - flag: "SecurityContextConstraint"
              set: true
            - flag: "anyuid"
            - flag: "hostaccess"
            - flag: "hostmount-anyuid"
            - flag: "hostnetwork"
            - flag: "node-exporter"
            - flag: "nonroot"
            - flag: "privileged"
            - flag: "restricted"
        remediation: |-
          在 OpenShift 4 中，无法禁用安全上下文约束（Security Context Constraint）准入控制器。
        scored: false

      - id: 1.2.14
        text: "确保 admission 控制插件 ServiceAccount 已设置为（手动）。"
        audit: |
          #Verify the list of admission controllers for 4.6 and higher
          oc -n openshift-kube-apiserver get configmap config -o json | jq -r '.data."config.yaml"' | jq '.apiServerArguments."enable-admission-plugins"'
          output=$(oc -n openshift-kube-apiserver get configmap config -o json | jq -r '.data."config.yaml"' | jq '.apiServerArguments."enable-admission-plugins"')
          [ "$output" == "null" ] && echo "ocp 4.5 has ServiceAccount compiled" || echo $output
          #Check that no overrides are configured
          oc get kubeapiservers.operator.openshift.io cluster -o json | jq -r '.spec.unsupportedConfigOverrides'
          #Verify that Service Accounts are present
          oc get sa -A
        tests:
          test_items:
            - flag: "ServiceAccount"
              set: true
        remediation: |-
          不需要任何配置。OpenShift 默认配置为使用服务账号。
        scored: false

      - id: 1.2.15
        text: "确保 admission control 插件 NamespaceLifecycle 已设置为 (手动)。"
        audit: |
          #Verify the list of admission controllers for 4.6 and higher
          oc -n openshift-kube-apiserver get configmap config -o json | jq -r '.data."config.yaml"' | jq '.apiServerArguments."enable-admission-plugins"'
          output=$(oc -n openshift-kube-apiserver get configmap config -o json | jq -r '.data."config.yaml"' | jq '.apiServerArguments."enable-admission-plugins"')
          [ "$output" == "null" ] && echo "ocp 4.5 has NamespaceLifecycle compiled" || echo $output
          #Check that no overrides are configured
          oc get kubeapiservers.operator.openshift.io cluster -o json | jq -r '.spec.unsupportedConfigOverrides'
        tests:
          test_items:
            - flag: "NamespaceLifecycle"
        remediation: |-
          确保 --disable-admission-plugins 参数不包含 NamespaceLifecycle。
        scored: false

      - id: 1.2.16
        text: "确保 Admission 控制插件 SecurityContextConstraint 已设置为（手动）。"
        audit: |
          #Verify the set of admission-plugins for OCP 4.6 and higher
          oc -n openshift-kube-apiserver get configmap config -o json | jq -r '.data."config.yaml"' | jq '.apiServerArguments."enable-admission-plugins"'
          output=$(oc -n openshift-kube-apiserver get configmap config -o json | jq -r '.data."config.yaml"' | jq '.apiServerArguments."enable-admission-plugins"')
          [ "$output" == "null" ] && echo "ocp 4.5 has SecurityContextConstraint compiled" || echo $output
          #Check that no overrides are configured
          oc get kubeapiservers.operator.openshift.io cluster -o json | jq -r '.spec.unsupportedConfigOverrides'
          #Verify that SecurityContextConstraints are deployed
          oc get scc
          oc describe scc restricted
        tests:
          bin_op: and
          test_items:
            - flag: "SecurityContextConstraint"
            - flag: "anyuid"
            - flag: "hostaccess"
            - flag: "hostmount-anyuid"
            - flag: "hostnetwork"
            - flag: "node-exporter"
            - flag: "nonroot"
            - flag: "privileged"
            - flag: "restricted"
        remediation: |-
          不需要任何操作。在 OpenShift 中，安全上下文约束默认启用且无法禁用。
        scored: false

      - id: 1.2.17
        text: "确保 admission 控制插件 NodeRestriction 已设置为 (手动)。"
        audit: |
          # For 4.5, review the control plane manifest https://github.com/openshift/origin/blob/release-4.5/vendor/k8s.io/kubernetes/cmd/kubeadm/app/phases/controlplane/manifests.go#L132
          #Verify the set of admission-plugins for OCP 4.6 and higher
          oc -n openshift-kube-apiserver get configmap config -o json | jq -r '.data."config.yaml"' | jq '.apiServerArguments."enable-admission-plugins"'
          output=$(oc -n openshift-kube-apiserver get configmap config -o json | jq -r '.data."config.yaml"' | jq '.apiServerArguments."enable-admission-plugins"')
          [ "$output" == "null" ] && echo "ocp 4.5 has NodeRestriction compiled" || echo $output
          #Check that no overrides are configured
          oc get kubeapiservers.operator.openshift.io cluster -o json | jq -r '.spec.unsupportedConfigOverrides'
        tests:
          test_items:
            - flag: "NodeRestriction"
        remediation: |-
          NodeRestriction 插件无法被禁用。
        scored: false

      - id: 1.2.18
        text: "确保未设置 --insecure-bind-address 参数（手动）。"
        audit: |
          # InsecureBindAddress=true should not be in the results
          oc get kubeapiservers.operator.openshift.io cluster -o jsonpath='{range .spec.observedConfig.apiServerArguments.feature-gates[*]}{@}{"\n"}{end}'
          # Result should be only 6443
          oc -n openshift-kube-apiserver get endpoints -o jsonpath='{.items[*].subsets[*].ports[*].port}'
          # Result should be only 8443
          oc -n openshift-apiserver get endpoints -o jsonpath='{.items[*].subsets[*].ports[*].port}'
        tests:
          bin_op: and
          test_items:
            - flag: "insecure-bind-address"
              set: false
            - flag: 6443
            - flag: 8443
        remediation: |-
          无需修改。
        scored: false

      - id: 1.2.19
        text: "确保将 --insecure-port 参数设置为 0（手动）。"
        audit: |
          # Should return 6443
          oc -n openshift-kube-apiserver get endpoints -o jsonpath='{.items[*].subsets[*].ports[*].port}'
          # For OCP 4.6 and above
          oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | jq '.apiServerArguments["insecure-port"]'
          output=$(oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | jq '.apiServerArguments["insecure-port"]')
          [ "$output" == "null" ] && echo "ocp 4.5 has insecure-port set to \"0\" compiled" || echo $output
        tests:
          bin_op: and
          test_items:
            - flag: "\"0\""
            - flag: "6443"
        remediation: |-
          不需要。该配置由 API 服务器操作员管理。
        scored: false

      - id: 1.2.20
        text: "确保 --secure-port 参数未设置为 0 (手动)。"
        audit: |
          oc get kubeapiservers.operator.openshift.io cluster -o json | jq '.spec.observedConfig'
          # Should return only 6443
          echo ports=`oc get pods -n openshift-kube-apiserver -l app=openshift-kube-apiserver -o jsonpath='{.items[*].spec.containers[?(@.name=="kube-apiserver")].ports[*].containerPort}'`
        tests:
          bin_op: and
          test_items:
            - flag: '"bindAddress": "0.0.0.0:6443"'
            - flag: "ports"
              compare:
                op: regex
                value: '\s*(?:6443\s*){1,}$'
        remediation: |-
          不需要。
        scored: false

      - id: 1.2.21
        text: "确保 healthz 端点受 RBAC 保护（手动）。"
        type: manual
        audit: |
          # Verify endpoints
          oc -n openshift-kube-apiserver describe endpoints
          # Check config for ports, livenessProbe, readinessProbe, healthz
          oc -n openshift-kube-apiserver get cm kube-apiserver-pod -o json | jq -r '.data."pod.yaml"' | jq '.spec.containers'
          # Test to validate RBAC enabled on the apiserver endpoint; check with non-admin role
          oc project openshift-kube-apiserver POD=$(oc get pods -n openshift-kube-apiserver -l app=openshift-kube-apiserver -o jsonpath='{.items[0].metadata.name}') PORT=$(oc get pods -n openshift-kube-apiserver -l app=openshift-kube-apiserver -o jsonpath='{.items[0].spec.containers[0].ports[0].hostPort}')
          # Following should return 403 Forbidden
          oc rsh -n openshift-kube-apiserver ${POD} curl https://localhost:${PORT}/metrics -k
          # Create a service account to test RBAC
          oc create -n openshift-kube-apiserver sa permission-test-sa
          # Should return 403 Forbidden
          SA_TOKEN=$(oc sa -n openshift-kube-apiserver get-token permission-test-sa)
          oc rsh -n openshift-kube-apiserver ${POD} curl https://localhost:${PORT}/metrics -H "Authorization: Bearer $SA_TOKEN" -k
          # Cleanup
          oc delete -n openshift-kube-apiserver sa permission-test-sa
          # As cluster admin, should succeed
          CLUSTER_ADMIN_TOKEN=$(oc whoami -t)
          oc rsh -n openshift-kube-apiserver ${POD} curl https://localhost:${PORT}/metrics -H "Authorization: Bearer $CLUSTER_ADMIN_TOKEN" -k
        remediation: |-
          不需要任何操作，因为性能分析数据受 RBAC 保护。
        scored: false

      - id: 1.2.22
        text: "确保设置了 --audit-log-path 参数（手动）。"
        audit: |
          # Should return “/var/log/kube-apiserver/audit.log"
          output=$(oc get configmap config -n openshift-kube-apiserver -o jsonpath="{['.data.config\.yaml']}" | jq '.auditConfig.auditFilePath')
          [ "$output" != "" ] && [ "$output" != "null" ] && echo "$output" || true
          output=$(oc get configmap config -n openshift-kube-apiserver -o json | jq -r '.data["config.yaml"]' | jq -r '.apiServerArguments["audit-log-path"][]?')
          [ "$output" != "" ] && [ "$output" != "null" ] && echo "$output" || true
          POD=$(oc get pods -n openshift-kube-apiserver -l app=openshift-kube-apiserver -o jsonpath='{.items[0].metadata.name}')
          oc rsh -n openshift-kube-apiserver -c kube-apiserver $POD ls /var/log/kube-apiserver/audit.log 2>/dev/null
          # Should return 0
          echo exit_code=$?
          # Should return "/var/log/openshift-apiserver/audit.log"
          output=$(oc get configmap config -n openshift-apiserver -o jsonpath="{['.data.config\.yaml']}" | jq '.auditConfig.auditFilePath')
          [ "$output" != "" ] && [ "$output" != "null" ] && echo "$output" || true
          output=$(oc get configmap config -n openshift-apiserver -o json | jq -r '.data["config.yaml"]' | jq -r '.apiServerArguments["audit-log-path"][]?')
          [ "$output" != "" ] && [ "$output" != "null" ] && echo "$output" || true
          POD=$(oc get pods -n openshift-apiserver -l apiserver=true -o jsonpath='{.items[0].metadata.name}')
          oc rsh -n openshift-apiserver $POD ls /var/log/openshift-apiserver/audit.log 2>/dev/null
          # Should return 0
          echo exit_code=$?
        use_multiple_values: true
        tests:
          bin_op: or
          test_items:
            - flag: "/var/log/kube-apiserver/audit.log"
            - flag: "/var/log/openshift-apiserver/audit.log"
            - flag: "exit_code=0"
            - flag: "null"
        remediation: |-
          不需要。这由集群 apiserver 操作员管理。
        scored: false

      - id: 1.2.23
        text: "确保审计日志被转发到集群外进行保留（手动方式）。"
        type: "manual"
        remediation: |-
          按照日志转发文档进行操作。将日志转发到第三方系统https://docs.openshift.com/container-platform/4.5/logging/cluster-logging-external.html
        scored: false

      - id: 1.2.24
        text: "确保将 maximumRetainedFiles 参数设置为 10 或适当的值（手动）。"
        audit: |
          #NOTICE
          output=$(oc get configmap config -n openshift-kube-apiserver -o json | jq -r '.data["config.yaml"]' | jq -r .auditConfig.maximumRetainedFiles)
          [ "$output" != "" ] && [ "$output" != "null" ] && echo "maximumRetainedFiles=$output" || true
          output=$(oc get configmap config -n openshift-apiserver -o json | jq -r '.data["config.yaml"]' | jq -r .auditConfig.maximumRetainedFiles)
          [ "$output" != "" ] && [ "$output" != "null" ] && echo "maximumRetainedFiles=$output" || true
          output=$(oc get configmap config -n openshift-kube-apiserver -o json | jq -r '.data["config.yaml"]' | jq -r '.apiServerArguments["audit-log-maxbackup"][]?')
          [ "$output" != "" ] && [ "$output" != "null" ] && echo "audit-log-maxbackup=$output" || true
          output=$(oc get configmap config -n openshift-apiserver -o json | jq -r '.data["config.yaml"]' | jq -r '.apiServerArguments["audit-log-maxbackup"][]?')
          [ "$output" != "" ] && [ "$output" != "null" ] && echo "audit-log-maxbackup=$output" || true
        use_multiple_values: true
        tests:
          bin_op: or
          test_items:
            - flag: "maximumRetainedFiles"
              compare:
                op: gte
                value: 10
            - flag: "audit-log-maxbackup"
              compare:
                op: gte
                value: 10
        remediation: |-
          将 maximumRetainedFiles 参数设置为 10 或适当的文件数。maximumRetainedFiles: 10
        scored: false

      - id: 1.2.25
        text: "确保将 maximumFileSizeMegabytes 参数设置为100或适当的数值（手动）。"
        audit: |
          #NOTICE
          output=$(oc get configmap config -n openshift-kube-apiserver -o json | jq -r '.data["config.yaml"]' | jq -r .auditConfig.maximumFileSizeMegabytes)
          [ "$output" != "" ] && [ "$output" != "null" ] && echo "maximumFileSizeMegabytes=$output" || true
          output=$(oc get configmap config -n openshift-apiserver -o json | jq -r '.data["config.yaml"]' | jq -r .auditConfig.maximumFileSizeMegabytes)
          [ "$output" != "" ] && [ "$output" != "null" ] && echo "maximumFileSizeMegabytes=$output" || true
          output=$(oc get configmap config -n openshift-kube-apiserver -o json | jq -r '.data["config.yaml"]' | jq -r '.apiServerArguments["audit-log-maxsize"][]?')
          [ "$output" != "" ] && [ "$output" != "null" ] && echo "audit-log-maxsize=$output" || true
          output=$(oc get configmap config -n openshift-apiserver -o json | jq -r '.data["config.yaml"]' | jq -r '.apiServerArguments["audit-log-maxsize"][]?')
          [ "$output" != "" ] && [ "$output" != "null" ] && echo "audit-log-maxsize=$output" || true
        use_multiple_values: true
        tests:
          bin_op: or
          test_items:
            - flag: "maximumFileSizeMegabytes"
              compare:
                op: gte
                value: 100
            - flag: "audit-log-maxsize"
              compare:
                op: gte
                value: 100
        remediation: |-
          将 audit-log-maxsize 参数设置为 100 或相应的合适数字。maximumFileSizeMegabytes: 100
        scored: false

      - id: 1.2.26
        text: "确保将 --request-timeout 参数设置为适当的数值（手动设置）。"
        audit: |
          echo requestTimeoutSeconds=`oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | jq -r .servingInfo.requestTimeoutSeconds`
        tests:
          test_items:
            - flag: "requestTimeoutSeconds"
        remediation: |-
          TBD 意味着 "待定"，通常用于表示某事情尚未确定或待定。
        scored: false

      - id: 1.2.27
        text: "确保将 --service-account-lookup 参数设置为 true（手动）。"
        audit: |
          # For OCP 4.5
          oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | jq '.apiServerArguments' | grep service-account-lookup
          # For OCP 4.6 and above
          oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | jq -r '.apiServerArguments["service-account-lookup"]'
          output=$(oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | jq -r '.apiServerArguments["service-account-lookup"][0]')
          [ "$output" == "null" ] && echo "ocp 4.5 has service-account-lookup=true compiled" || echo service-account-lookup=$output
        tests:
          test_items:
            - flag: "service-account-lookup=true"
        remediation: |-
          TBD（To be determined）- 待定
        scored: false

      - id: 1.2.28
        text: "确保将 --service-account-key-file 参数设置为适当的值（手动）。"
        audit: |
          oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | jq -r .serviceAccountPublicKeyFiles[]
        tests:
          bin_op: and
          test_items:
            - flag: "/etc/kubernetes/static-pod-resources/configmaps/sa-token-signing-certs"
            - flag: "/etc/kubernetes/static-pod-resources/configmaps/bound-sa-token-signing-certs"
        remediation: |-
          OpenShift API 服务器不使用 service-account-key-file 参数。ServiceAccount token authenticator 是通过 serviceAccountConfig.publicKeyFiles 配置的。OpenShift 不会重用 apiserver TLS 密钥。这是不可配置的。
        scored: false

      - id: 1.2.29
        text: "确保将 --etcd-certfile 和 --etcd-keyfile 参数适当设置为正确的值（手动）。"
        audit: |
          # etcd Certificate File
          oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | jq -r .storageConfig.certFile
          # etcd Key File
          oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | jq -r .storageConfig.keyFile
          # NOTICE 4.6 extention
          oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | jq -r '.apiServerArguments["etcd-certfile"]'
          oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | jq -r '.apiServerArguments["etcd-keyfile"]'
        tests:
          bin_op: and
          test_items:
            - flag: "/etc/kubernetes/static-pod-resources/secrets/etcd-client/tls.crt"
            - flag: "/etc/kubernetes/static-pod-resources/secrets/etcd-client/tls.key"
        remediation: |-
          OpenShift 会自动管理 etcd 的 TLS 和客户端证书认证，这是不可配置的。
        scored: false

      - id: 1.2.30
        text: "确保将 --tls-cert-file 和 --tls-private-key-file 参数设置为适当的数值（手动）。"
        audit: |
          # TLS Cert File - openshift-kube-apiserver
          oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | jq -r .servingInfo.certFile
          # TLS Key File
          oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | jq -r '.servingInfo.keyFile'
          # NOTECI 4.6 extention
          oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | jq -r '.apiServerArguments["tls-cert-file"]'
          oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | jq -r '.apiServerArguments["tls-private-key-file"]'
        tests:
          bin_op: and
          test_items:
            - flag: "/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.crt"
            - flag: "/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.key"
        remediation: |-
          OpenShift会自动管理API服务器与节点/Kubelet之间的TLS认证。这是不可配置的。您可以选择设置一个自定义的默认证书，供API服务器在提供内容时使用，以便使客户端可以在不需要分发集群管理的证书颁发机构（CA）证书的情况下，通过不同的主机名访问API服务器。请按照OpenShift文档中的指引User-provided certificates for the API server进行操作。
        scored: false

      - id: 1.2.31
        text: "确保 --client-ca-file 参数被适当设置（手动）。"
        audit: |
          oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | jq -r .servingInfo.clientCA
          oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | jq -r '.apiServerArguments["client-ca-file"]'
        tests:
          test_items:
            - flag: "/etc/kubernetes/static-pod-certs/configmaps/client-ca/ca-bundle.crt"
        remediation: |-
          OpenShift 自动管理 API 服务器与节点 / Kubelet 通信的 TLS 认证。这是不可配置的。您可以选择设置一个自定义默认证书，以便在提供内容时由 API 服务器使用，以便让客户端可以通过不同的主机名访问 API 服务器或者无需将集群管理的证书颁发机构（CA）证书分发给客户端。用户提供的证书必须在 openshift-config 命名空间中以 kubernetes.io/tls 类型的 Secret 提供。更新 API 服务器集群配置，即 apiserver/cluster 资源，以启用使用用户提供的证书。
        scored: false

      - id: 1.2.32
        text: "确保将 --etcd-cafile 参数设置为适当的值（手动）。"
        audit: |
          #etcd CA File
          oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | jq -r .storageConfig.ca
          oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | jq -r '.apiServerArguments["etcd-cafile"]'
        tests:
          test_items:
            - flag: "/etc/kubernetes/static-pod-resources/configmaps/etcd-serving-ca/ca-bundle.crt"
        remediation: |-
          不需要任何操作。OpenShift 会生成 etcd-cafile 并在 API 服务器中适当设置参数。与 etcd 的通信由 etcd 服务 CA 进行安全保护。
        scored: false

      - id: 1.2.33
        text: "确保 --encryption-provider-config 参数设置正确（手动）。"
        audit: |
          # encrypt the etcd datastore
          oc get openshiftapiserver -o=jsonpath='{range.items[0].status.conditions[?(@.type=="Encrypted")]}{.reason}{"\n"}{.message}{"\n"}'
        tests:
          test_items:
            - flag: "EncryptionCompleted"
        remediation: |-
          请按照 OpenShift 文档中的指引进行 etcd 数据加密和身份认证操作。您可以参考以下链接：https://docs.openshift.com/container-platform/4.5/security/encrypting-etcd.html
        scored: false

      - id: 1.2.34
        text: "确保加密提供程序被适当配置（手动）。"
        audit: |
          # encrypt the etcd datastore
          oc get openshiftapiserver -o=jsonpath='{range.items[0].status.conditions[?(@.type=="Encrypted")]}{.reason}{"\n"}{.message}{"\n"}'
        tests:
          test_items:
            - flag: "EncryptionCompleted"
        remediation: |-
          按照 Kubernetes 文档的指导，配置一个 EncryptionConfig 文件。在该文件中，选择 aescbc、kms 或 secretbox 作为加密提供程序。
        scored: false

      - id: 1.2.35
        text: "确保 API Server 仅使用强加密密码（手动）。"
        type: manual
        audit: |
          # verify cipher suites
          oc get cm -n openshift-authentication v4-0-config-system-cliconfig -o jsonpath='{.data.v4\-0\-config\-system\-cliconfig}' | jq .servingInfo
          oc get kubeapiservers.operator.openshift.io cluster -o json |jq.spec.observedConfig.servingInfo
          oc get openshiftapiservers.operator.openshift.io cluster -o json |jq.spec.observedConfig.servingInfo
          oc describe --namespace=openshift-ingress-operator ingresscontroller/default
        remediation: |-
          验证 tlsSecurityProfile 是否设置为您选择的值。注意：HAProxy Ingress 控制器镜像不支持 TLS 1.3，因为 Modern profile 需要 TLS 1.3，所以不受支持。Ingress Operator 会将 Modern profile 转换为 Intermediate。Ingress Operator 还会将 Old 或 Custom profile 的 TLS 1.0 转换为 1.1，将 Custom profile 的 TLS 1.3 转换为 1.2。
        scored: false

  - id: 1.3
    text: "控制器管理器"
    checks:
      - id: 1.3.1
        text: "确保垃圾回收已配置为适当的方式（手动）。"
        type: manual
        remediation: |-
          要进行配置，请按照《为容器和镜像配置垃圾回收》中的指示进行操作。链接：https://docs.openshift.com/container-platform/4.5/nodes/nodes/nodes-nodes-garbage-collection.html#nodes-nodes-garbage-collection-configuring_nodes-nodes-configuring
        scored: false

      - id: 1.3.2
        text: "确保控制器管理器的健康检查端点受到 RBAC 的保护（手动操作）。"
        type: manual
        audit: |
          # Verify configuration for ports, livenessProbe, readinessProbe, healthz
          oc -n openshift-kube-controller-manager get cm kube-controller-manager-pod -o json | jq -r '.data."pod.yaml"' | jq '.spec.containers'
          # Verify endpoints
          oc -n openshift-kube-controller-manager describe endpoints
          # Test to validate RBAC enabled on the controller endpoint; check with non-admin role
          oc project openshift-kube-controller-manage
          POD=$(oc get pods -n openshift-kube-controller-manager -l app=kube-controller-manager -o jsonpath='{.items[0].metadata.name}')
          PORT=$(oc get pods -n openshift-kube-controller-manager -l app=kube-controller-manager -o jsonpath='{.items[0].spec.containers[0].ports[0].hostPort}')
          # Following should return 403 Forbidden
          oc rsh -n openshift-kube-controller-manager ${POD} curl https://localhost:${PORT}/metrics -k
          # Create a service account to test RBAC
          oc create -n openshift-kube-controller-manager sa permission-test-sa
          # Should return 403 Forbidden
          SA_TOKEN=$(oc sa -n openshift-kube-controller-manager get-token permission-test-sa)
          oc rsh -n openshift-kube-controller-manager ${POD} curl https://localhost:${PORT}/metrics -H "Authorization: Bearer $SA_TOKEN" -k
          # Cleanup
          oc delete -n openshift-kube-controller-manager sa permission-test-sa
          # As cluster admin, should succeed
          CLUSTER_ADMIN_TOKEN=$(oc whoami -t)
          oc rsh -n openshift-kube-controller-manager ${POD} curl https://localhost:${PORT}/metrics -H "Authorization: Bearer $CLUSTER_ADMIN_TOKEN" -k
        remediation: |-
          不需要任何操作；性能分析由 RBAC 保护。
        scored: false

      - id: 1.3.3
        text: "确保将 --use-service-account-credentials 参数设置为 true（手动）。"
        audit: |
          echo use-service-account-credentials=`oc get configmaps config -n openshift-kube-controller-manager -ojson | jq -r '.data["config.yaml"]' | jq -r '.extendedArguments["use-service-account-credentials"][]'`
        tests:
          test_items:
            - flag: "use-service-account-credentials"
              compare:
                op: eq
                value: true
        remediation: |-
          OpenShift 控制器管理器运算符负责管理和更新 OpenShift 控制器管理器。Kubernetes 控制器管理器运算符负责管理和更新部署在 OpenShift 之上的 Kubernetes 控制器管理器。此运算符通过 KubeControllerManager 自定义资源进行配置。
        scored: false

      - id: 1.3.4
        text: "确保将 --service-account-private-key-file 参数设置为适当的值（手动设置）。"
        audit: |
          oc get configmaps config -n openshift-kube-controller-manager -ojson | jq -r '.data["config.yaml"]' | jq -r '.extendedArguments["service-account-private-key-file"][]'
        tests:
          test_items:
            - flag: "/etc/kubernetes/static-pod-resources/secrets/service-account-private-key/service-account.key"
        remediation: |-
          不需要。OpenShift 会自动管理调度器的服务账号凭据。
        scored: false

      - id: 1.3.5
        text: "确保将 --root-ca-file 参数设置为适当的值（手动）。"
        audit: |
          oc get configmaps config -n openshift-kube-controller-manager -ojson | jq -r '.data["config.yaml"]' | jq -r '.extendedArguments["root-ca-file"][]'
        tests:
          test_items:
            - flag: "/etc/kubernetes/static-pod-resources/configmaps/serviceaccount-ca/ca-bundle.crt"
        remediation: |-
          不需要任何操作。OpenShift 平台组件的证书会被 OpenShift 容器平台自动创建和轮换。
        scored: false

      - id: 1.3.6
        text: "确保 RotateKubeletServerCertificate 参数设置为 true（手动）。"
        audit: |
          oc get configmaps config -n openshift-kube-controller-manager -ojson | jq -r '.data["config.yaml"]' | jq -r '.extendedArguments["feature-gates"][]'
        tests:
          test_items:
            - flag: "RotateKubeletServerCertificate"
              compare:
                op: eq
                value: "true"
        remediation: |-
          不需要额外操作。OpenShift 平台组件的证书会被 OpenShift 容器平台自动创建和轮换。
        scored: false

      - id: 1.3.7
        text: "确保将 --bind-address 参数设置为 127.0.0.1（手动设置）。"
        audit: |
          echo port=`oc get configmaps config -n openshift-kube-controller-manager -ojson | jq -r '.data["config.yaml"]' | jq '.extendedArguments["port"][]'`
          echo secure-port=`oc get configmaps config -n openshift-kube-controller-manager -ojson | jq -r '.data["config.yaml"]' | jq '.extendedArguments["secure-port"][]'`
          #Following should fail with a http code 403
          POD=$(oc get pods -n openshift-kube-controller-manager -l app=kube-controller-manager -o jsonpath='{.items[0].metadata.name}')
          oc rsh -n openshift-kube-controller-manager -c kube-controller-manager $POD curl https://localhost:10257/metrics -k
        tests:
          bin_op: and
          test_items:
            - flag: "secure-port"
              compare:
                op: eq
                value: "\"10257\""
            - flag: "port"
              compare:
                op: eq
                value: "\"0\""
            - flag: "\"code\": 403"
        remediation: |-
          编辑主节点上的 Controller Manager pod 规范文件 $controllermanagerconf，并确保 --bind-address 参数的正确数值。
        scored: false

  - id: 1.4
    text: "调度器"
    checks:
      - id: 1.4.1
        text: "确保调度器的 healthz 端点受到 RBAC 的保护（手动配置）。"
        type: manual
        audit: |
          # check configuration for ports, livenessProbe, readinessProbe, healthz
          oc -n openshift-kube-scheduler get cm kube-scheduler-pod -o json | jq -r '.data."pod.yaml"' | jq '.spec.containers'
          # Test to verify endpoints
          oc -n openshift-kube-scheduler describe endpoints
          # Test to validate RBAC enabled on the scheduler endpoint; check with non-admin role
          oc project openshift-kube-scheduler
          POD=$(oc get pods -l app=openshift-kube-scheduler -o jsonpath='{.items[0].metadata.name}')
          PORT=$(oc get pod $POD -o jsonpath='{.spec.containers[0].livenessProbe.httpGet.port}')
          # Should return 403 Forbidden
          oc rsh ${POD} curl http://localhost:${PORT}/metrics -k
          # Create a service account to test RBAC
          oc create sa permission-test-sa
          # Should return 403 Forbidden
          SA_TOKEN=$(oc sa get-token permission-test-sa)
          oc rsh ${POD} curl http://localhost:${PORT}/metrics -H "Authorization: Bearer $SA_TOKEN" -k
          # Cleanup
          oc delete sa permission-test-sa
          # As cluster admin, should succeed
          CLUSTER_ADMIN_TOKEN=$(oc whoami -t)
          oc rsh ${POD} curl http://localhost:${PORT}/metrics -H "Authorization: Bearer $CLUSTER_ADMIN_TOKEN" -k
        remediation: |-
          这个问题的修复：https://bugzilla.redhat.com/show_bug.cgi?id=1889488 不需要。性能分析受 RBAC 保护，无法禁用。
        scored: false

      - id: 1.4.2
        text: "验证调度器 API 服务是否受到身份验证和授权的保护（手动进行）。"
        type: manual
        audit: |
          # To verify endpoints
          oc -n openshift-kube-scheduler describe endpoints
          # To verify that bind-adress is not used in the configuration and that port is set to 0
          oc -n openshift-kube-scheduler get cm kube-scheduler-pod -o json | jq -r '.data."pod.yaml"' | jq '.spec.containers'
          # To test for RBAC:
          oc project openshift-kube-scheduler
          POD=$(oc get pods -l app=openshift-kube-scheduler -o jsonpath='{.items[0].metadata.name}')
          POD_IP=$(oc get pods -l app=openshift-kube-scheduler -o jsonpath='{.items[0].status.podIP}')
          PORT=$(oc get pod $POD -o jsonpath='{.spec.containers[0].livenessProbe.httpGet.port}')
          # Should return a 403
          oc rsh ${POD} curl http://${POD_IP}:${PORT}/metrics
          # Create a service account to test RBAC
          oc create sa permission-test-sa
          # Should return 403 Forbidden
          SA_TOKEN=$(oc sa get-token permission-test-sa)
          oc rsh ${POD} curl http://localhost:${PORT}/metrics -H "Authorization: Bearer $SA_TOKEN" -k
          # Cleanup
          oc delete sa permission-test-sa
          # As cluster admin, should succeed
          CLUSTER_ADMIN_TOKEN=$(oc whoami -t)
          oc rsh ${POD} curl http://localhost:${PORT}/metrics -H "Authorization: Bearer $CLUSTER_ADMIN_TOKEN" -k
        remediation: |-
          默认情况下，--bind-address 参数不存在，readinessProbe 和 livenessProbe 参数分别设置为 10251，port 参数设置为 0。请查看此问题的状态： https://bugzilla.redhat.com/show_bug.cgi?id=1889488
        scored: false
